{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_data[0])\n",
    "print(train_targets)\n",
    "\n",
    "# It would be problematic to feed into a neural network values that all take wildly different ranges.\n",
    "# The network might be able to automatically adapt to such heterogeneous\n",
    "# data, but it would definitely make learning more difficult\n",
    "\n",
    "# feature-wise normalization\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "    input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# k-cross validation\n",
    "# It consists of splitting the available data into K partitions (typically K = 4 or 5),\n",
    "# instantiating K identical models, and training each one on K â€“ 1 partitions while evaluating on\n",
    "# the remaining partition. The validation score for the model used is then the average of\n",
    "# the K validation scores obtained\n",
    "\n",
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "all_mae_histories = []\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "    [train_data[:i * num_val_samples],\n",
    "    train_data[(i + 1) * num_val_samples:]],\n",
    "    axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "    [train_targets[:i * num_val_samples],\n",
    "    train_targets[(i + 1) * num_val_samples:]],\n",
    "    axis=0)\n",
    "    model = build_model()\n",
    "    # model.fit(partial_train_data, partial_train_targets,\n",
    "    #           epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    # all_scores.append(val_mae)\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)\n",
    "    average_mae_history = [\n",
    "        np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "             previous = smoothed_points[-1]\n",
    "             smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "        return smoothed_points\n",
    "\n",
    "average_mae_history = [\n",
    "np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "# According to this plot, validation MAE stops improving significantly after 80 epochs.\n",
    "# Past that point, you start overfitting so we use eopchs=80\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_data, train_targets,\n",
    "epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "\n",
    "print(test_mse_score)\n",
    "print(test_mae_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
